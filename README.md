# reinforcement-learning

<h4>Upper Confidence Bound (UCB)</h4> is the most widely used solution method for multi-armed bandit problems. This algorithm is based on the principle of optimism in the face of uncertainty. In other words, the more uncertain we are about an arm, the more important it becomes to explore that arm.
<br/>

<h4>Thompson Sampling (Posterior Sampling or Probability Matching</h4> is an algorithm for choosing the actions that address the exploration-exploitation dilemma in multi-armed bandit problem. Actions are performed several times and are called exploration. It uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit trial-and-error search for good behaviour. 
